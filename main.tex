\documentclass{article}

\input{header}

\title{DSGA 1011: Assignment 4}

\author{Megh Rathod \\ Net ID: mr7375}

\date{November 12, 2025}


\colmfinalcopy
\begin{document}
\maketitle
% \section*{Part I. Q1} No written element, submit \texttt{out\_original.txt}  to autograder.
\section*{Q0. 1.}
Please provide a link to your github repository, which contains the code for both Part I and Part II. 

\url{https://github.com/meghrathod/nlp-ass-5}
\section*{Q2. 1.}
Describe your transformation of dataset.

My transformation strategy creates out-of-distribution (OOD) evaluation data by applying multiple realistic text modifications that preserve semantic meaning and sentiment labels. The transformation uses a multi-stage approach combining synonym replacement, verb tense changes, contraction expansion/contraction, and controlled typographical errors.

\subsection*{Transformation Components}

\textbf{1. Synonym Replacement (Two-Tier Approach):}
I employ a two-tier synonym replacement strategy with part-of-speech (POS) based probabilities:
\begin{itemize}
    \item \textbf{Domain-Specific Groups (Primary):} I maintain curated synonym groups for movie review terminology, including:
    \begin{itemize}
        \item Movie/film terms: \texttt{\{movie, film, picture, motion picture\}}
        \item Acting/performance: \texttt{\{acting, performance, portrayal, playing\}}
        \item Story/narrative: \texttt{\{story, narrative, tale, plot, screenplay, script\}}
        \item Quality adjectives: \texttt{\{good, great, excellent, fine, fantastic, outstanding\}} and \texttt{\{bad, terrible, awful, poor, horrible\}}
        \item Common verbs/adverbs: \texttt{\{watch, see\}}, \texttt{\{very, extremely, really, quite\}}
    \end{itemize}
    These groups are stored as sets and automatically build bidirectional lookup dictionaries, ensuring no redundancy and easy maintenance.
    
    \item \textbf{WordNet Fallback:} For words not in domain groups, I query WordNet synsets using NLTK's \texttt{wordnet.synsets()} with appropriate POS tags. I filter synonyms to ensure they are alphabetic, differ from the original word, and are not duplicates.
    
    \item \textbf{POS-Based Probabilities:} Replacement probabilities vary by word type to balance semantic preservation with transformation aggressiveness:
    \begin{itemize}
        \item Adjectives: 80\% (\texttt{p\_synonym\_adj = 0.80})
        \item Adverbs: 80\% (\texttt{p\_synonym\_adv = 0.80})
        \item Nouns: 65\% (\texttt{p\_synonym\_noun = 0.65})
        \item Verbs: 80\% (\texttt{p\_synonym\_verb = 0.80})
    \end{itemize}
    Higher probabilities for adjectives/adverbs allow semantic variation while maintaining sentiment, while verbs and nouns are transformed more conservatively to preserve core meaning.
\end{itemize}

\textbf{2. Verb Tense Changes:}
With probability 75\% (\texttt{p\_tense\_change = 0.75}), I change verb tenses using a comprehensive mapping dictionary covering common verb forms (e.g., present $\leftrightarrow$ past: \texttt{is $\leftrightarrow$ was}, \texttt{goes $\leftrightarrow$ went}, \texttt{watches $\leftrightarrow$ watched}). This introduces temporal variation while preserving semantic content.

\textbf{3. Contraction Expansion/Contraction:}
With probability 20\% (\texttt{p\_contraction = 0.20}), I randomly expand contractions (e.g., \texttt{don't} $\rightarrow$ \texttt{do not}) or contract expanded forms (e.g., \texttt{I am} $\rightarrow$ \texttt{I'm}). This simulates natural language variation in formality and style.

\textbf{4. Controlled Typographical Errors:}
I introduce realistic typing errors using QWERTY keyboard proximity simulation:
\begin{itemize}
    \item Probability: 40\% per eligible word (\texttt{p\_typo = 0.40})
    \item Maximum: 10 typos per example (\texttt{max\_typos\_per\_example = 10})
    \item Method: For words longer than 3 characters, I randomly select a character position (excluding first and last) and replace it with a nearby QWERTY key. For example, \texttt{a} can be replaced by \texttt{s}, \texttt{q}, \texttt{w}, or \texttt{e}; \texttt{e} by \texttt{w}, \texttt{r}, \texttt{d}, or \texttt{s}.
    \item Constraint: Typos are only introduced if the word was not already transformed by synonym replacement, ensuring transformations don't compound.
\end{itemize}

\subsection*{Implementation Details}

The transformation pipeline processes text as follows:
\begin{enumerate}
    \item \textbf{Tokenization:} Use NLTK's \texttt{word\_tokenize()} to preserve sentence structure
    \item \textbf{POS Tagging:} Tag tokens with NLTK's \texttt{pos\_tag()} to identify word types
    \item \textbf{Transformation Stages:} Apply transformations in order: (1) contraction handling, (2) synonym replacement (domain groups then WordNet), (3) verb tense changes, (4) typo introduction
    \item \textbf{Capitalization Preservation:} All replacements maintain the original word's capitalization pattern (uppercase, lowercase, title case)
    \item \textbf{Detokenization:} Reconstruct sentences using \texttt{TreebankWordDetokenizer()} to restore natural spacing and punctuation
\end{enumerate}

\subsection*{Why This Transformation is Reasonable}

\begin{itemize}
    \item \textbf{Realistic Variations:} Synonym usage, tense changes, contractions, and typos all occur naturally in user-generated content, especially in informal movie reviews.
    \item \textbf{Preserves Semantics:} All transformations maintain the core meaning and sentiment of the original text, ensuring labels remain valid.
    \item \textbf{Appropriate Complexity:} The transformation is non-trivial (requires semantic understanding) but not extreme (text remains readable and interpretable).
    \item \textbf{Test-Time Plausibility:} These variations are commonly observed in real-world test scenarios, making the OOD evaluation meaningful.
\end{itemize}

\subsection*{Example Transformation}

\textbf{Original:} ``Titanic is the best movie I have ever seen. The acting was excellent and the story was compelling.''

\textbf{Transformed:} ``Titanic was the finest film I have ever seen. The performance was outstanding and the narrative was compelling.''

\textbf{Changes:} \texttt{is} $\rightarrow$ \texttt{was} (tense), \texttt{best} $\rightarrow$ \texttt{finest} (synonym), \texttt{movie} $\rightarrow$ \texttt{film} (domain group), \texttt{acting} $\rightarrow$ \texttt{performance} (domain group), \texttt{excellent} $\rightarrow$ \texttt{outstanding} (domain group), \texttt{story} $\rightarrow$ \texttt{narrative} (domain group).
% \section*{Part I. Q2. 2. No written element, submit \texttt{out\_transformed.txt} to autograder. }
\section*{Q3. 1}
\textbf{Report \& Analysis}

\subsection*{Accuracy Results}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Model & Original Test Set & Transformed Test Set \\
\midrule
Baseline (no augmentation) & 93.57\% & 84.95\% \\
With data augmentation & 93.61\% & 90.13\% \\
\bottomrule
\end{tabular}
\caption{Accuracy on original and transformed test sets.}
\label{tab:accuracy_results}
\end{table}

\subsection*{Analysis and Discussion}

\textbf{(1) Model Performance on Transformed Test Data After Augmentation:}

After applying data augmentation during training, the model's performance on the transformed test data \textbf{improved} from 84.95\% to 90.13\%, representing a gain of 5.18 percentage points. This suggests that the augmentation strategy successfully exposed the model to similar transformations during training, improving robustness to lexical variations, typos, and other surface-level changes while maintaining semantic understanding.

\textbf{(2) Impact of Data Augmentation on Original Test Data:}

Data augmentation \textbf{slightly enhanced} the model's performance on the original test set from 93.57\% to 93.61\%, a marginal improvement of 0.04 percentage points. This indicates that the augmented training data helped the model learn more generalizable features that also benefit performance on standard test data, without introducing significant distribution shift that would hurt performance on the original distribution.

\subsection*{Intuitive Explanation}

The observed results can be explained by considering how data augmentation affects model training:

\begin{itemize}
    \item \textbf{Improved OOD Robustness:} When the model is trained on augmented data that includes synonym replacements, tense changes, contractions, and typos, it learns to focus on semantic patterns rather than memorizing specific lexical choices. This makes it more robust to the lexical variations present in the transformed test set.
    
    \item \textbf{Regularization Effect:} The augmented training examples act as a form of regularization, preventing the model from overfitting to exact word sequences and encouraging it to learn more generalizable representations.
    
    \item \textbf{Trade-off with Original Distribution:} If augmentation is too aggressive or introduces patterns not present in the original distribution, it may hurt performance on the original test set. Conversely, if augmentation is well-calibrated, it can improve generalization to both original and transformed data.
    
    \item \textbf{Semantic Preservation:} Since our transformations preserve semantic meaning and sentiment, the model learns that different surface forms can express the same underlying sentiment, improving its ability to handle natural language variation.
\end{itemize}

\subsection*{Limitation of the Data Augmentation Approach}

One key limitation of this data augmentation approach for improving OOD performance is that it only addresses \textbf{lexical and surface-level variations} (synonyms, typos, contractions, tense changes) but does not handle \textbf{semantic or structural distribution shifts}. For example:

\begin{itemize}
    \item The augmentation cannot help with domain shifts where the underlying sentiment indicators change (e.g., reviews from a different genre or cultural context where ``dark'' might be positive rather than negative).
    
    \item It does not address cases where the transformation introduces subtle semantic shifts that my heuristics fail to detect, potentially creating label noise.
    
    \item The approach assumes that synonym replacement and other transformations perfectly preserve sentiment, which may not always hold true in practice (e.g., ``terrible'' vs. ``poor'' may have different intensity levels).
    
    \item The augmentation is limited to transformations I can programmatically define, missing more complex linguistic variations like paraphrasing, style changes, or discourse-level modifications that occur in real OOD scenarios.
\end{itemize}

To truly improve OOD robustness, I would need to combine lexical augmentation with other techniques such as adversarial training, domain adaptation, or learning from diverse data sources that capture broader distribution shifts.
\section*{Part II. Q4}
% 
% \section{Data Statistics and Processing (8pt)}


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
Number of examples & 4225 & 466 \\
Mean sentence length & 10.96 & 10.91 \\
Mean SQL query length & 60.90 & 58.90  \\
Vocabulary size (natural language)& 868 & 444  \\
Vocabulary size (SQL)& 644 & 393  \\
\bottomrule
\end{tabular}
\caption{Data statistics before any pre-processing.}
\label{tab:data_stats_before}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
\multicolumn{3}{l}{\textbf{T5 fine-tuned model (T5TokenizerFast)}} \\
Mean NL input length (tokens, no prompt) & 18.10 & 18.07 \\
Mean NL input length (tokens, with prompt) & 23.10 & 23.07 \\
Mean SQL query length (tokens) & 217.37 & 211.05 \\
Vocabulary size (natural language, token IDs) & 796 & 470 \\
Vocabulary size (SQL, token IDs) & 556 & 396 \\
\bottomrule
\end{tabular}
\caption{Data statistics after T5 tokenization. The prompt ``translate English to SQL: '' adds 5 tokens to each input. Token statistics include special tokens (BOS/EOS).}
\label{tab:data_stats_after}
\end{table}



\newpage




\section*{Q5}\label{sec:t5}


\begin{table}[h!]
\centering
\begin{tabular}{p{3.5cm}p{10cm}}
\toprule
Design choice & Description \\
\midrule
Data processing & Natural language questions and SQL queries are loaded without additional preprocessing. The database schema is provided in the input prompt to help the model understand table/column relationships. No data augmentation was applied to the training set. \\
\midrule
Tokenization & I use the default T5TokenizerFast from HuggingFace (\texttt{google-t5/t5-small}). Encoder input format: ``translate English to SQL: [natural language question]''. Decoder receives teacher-forced SQL tokens during training. Maximum sequence lengths: 512 tokens for both encoder and decoder. \\
\midrule
Architecture & Fine-tuned the entire T5-small model (60M parameters) including both encoder and decoder layers. No layers were frozen. Gradient checkpointing was available but not used in final model (sufficient memory with batch size 32). Mixed precision (FP16) training was enabled for 2x speedup. \\
\midrule
Hyperparameters & \textbf{Phase 1 (Cold Start, Epochs 0-19):} LR=5e-5, batch size=16, gradient accumulation=2 (effective batch=32), cosine scheduler with 2 warmup epochs, weight decay=0.01, patience=5 epochs. Achieved 36.66\% F1. \\
& \textbf{Phase 2 (Ultra-Low LR Fine-tuning):} \textbf{LR=2e-6} (constant, 25x lower), batch size=48, gradient accumulation=1 (effective batch=48), weight decay=0.0001, patience=1 epoch. This ultra-low LR was critical for staying near the optimal checkpoint while making microscopic improvements. Achieved 63.93\% F1. \\
& \textbf{Optimization:} AdamW optimizer, mixed precision (FP16), eval beam size=5. Hardware: L4 GPU (24GB VRAM). \\
\bottomrule
\end{tabular}
\caption{Details of the best-performing T5 model configurations (fine-tuned)}
\label{tab:t5_results_ft}
\end{table}

\subsection*{Key Training Insights and Design Decisions}

\paragraph{1. Critical Discovery - Checkpoint Proximity and Ultra-Low LR:}
When resuming from a well-trained checkpoint, we discovered the model was already near-optimal. Multiple experiments revealed:
\begin{itemize}
    \item \textbf{LR=5e-5 with warmup:} 53.1\% → 41.1\% F1 (overshoot and crash)
    \item \textbf{LR=1e-5 constant:} 58.6\% → 38.7\% F1 (still too high, moved away from optimum)
    \item \textbf{LR=2e-6 constant:} 63.2\% → 63.9\% F1 (success! Microscopic adjustments preserved optimum)
\end{itemize}
\textbf{Key Insight:} The checkpoint was in a sharp, high-quality local optimum. Ultra-low LR (2e-6, 25x lower than initial training) allowed the model to explore the immediate neighborhood without falling into worse regions.

\paragraph{2. Memory and Speed Optimizations:}
\begin{itemize}
    \item \textbf{Mixed Precision (FP16):} Enabled 2x speedup and reduced memory from ~12GB to ~6GB per batch
    \item \textbf{Batch Size Scaling:} Started with batch size 16 (effective 32 with grad accumulation) in Phase 1, scaled to 48 in Phase 2 when memory allowed
    \item \textbf{Fast Evaluation Strategy:} Full evaluation with beam search generation takes ~3 minutes per epoch. Implemented ``fast eval'' (loss-only, no generation) for non-checkpoint epochs, reducing total eval time by ~60\%
    \item \textbf{Gradient Accumulation:} Used in Phase 1 (accumulation steps=2) to achieve effective batch size of 32; Phase 2 used direct batch size of 48
\end{itemize}

\paragraph{3. Two-Phase Training Strategy:}
\begin{itemize}
    \item \textbf{Phase 1 (Cold Start):} Higher LR (5e-5), cosine scheduler with warmup, aggressive weight decay (0.01) → Achieved 36.66\% F1 by epoch 14
    \item \textbf{Phase 2 (Ultra-Low LR Fine-tuning):} Ultra-low constant LR (2e-6, 25x lower than Phase 1), no scheduler, reduced weight decay (0.0001), higher patience → Prevents overshoot, enables stable improvement from good checkpoint, achieved 63.93\% F1
\end{itemize}

\paragraph{4. Hardware Considerations:}
On L4 GPU (24GB VRAM): ~1.5 min/epoch with optimizations (mixed precision, batch size 48). The L4's higher memory capacity compared to T4 allowed for larger batch sizes, contributing to faster training throughput.




\section*{Q6. }

\paragraph{Quantitative Results:} 
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
  \toprule
  System & Record EM (\%) & Record F1 (\%)\\
  \midrule
  \multicolumn{3}{l}{\textbf{Dev Results}} \\
  \midrule
  
  \multicolumn{3}{l}{\textbf{T5 fine-tuned (Phase 1: Epochs 0-19)}} \\
  Best checkpoint (Epoch 14) & 29.61 & 36.66 \\
  End of Phase 1 (Epoch 19) & 23.39 & 27.72 \\[5pt]
  
  \multicolumn{3}{l}{\textbf{T5 fine-tuned (Phase 2: Ultra-Low LR)}} \\
  Epoch 0 (LR=2e-6) & 55.79 & 63.16 \\
  Epoch 1 & 56.44 & 63.87 \\
  Epoch 2 & 56.44 & 63.93 \\[5pt]
  
  \midrule
  \multicolumn{3}{l}{\textbf{Best Model Results}} \\
  \midrule
  Development set (Phase 2, Epoch 2) & 56.44 & 63.93 \\
  Test set (Autograder) & 51.85 & 58.99 \\
  \bottomrule
\end{tabular}  
\caption{Development and test set results. Phase 1: LR=5e-5 with cosine scheduling achieved 36.66\% F1. Phase 2: Ultra-low constant LR=2e-6 (critical discovery) achieved 63.93\% F1 on dev set. Test set F1 (autograder): 58.99\%. The 5\% performance gap between dev and test suggests some overfitting to dev set or slight distribution shift. Note: Record-based metrics focus on database query result accuracy rather than SQL syntax matching.}
\label{tab:results}
\end{table}

\paragraph{Training Progress Notes:}
Initial training (Phase 1) achieved steady improvement from 0\% to 36.66\% F1 over 14 epochs, then began overfitting. Multiple attempts to resume training revealed a critical insight: the checkpoint was already near-optimal, and any significant learning rate caused degradation. 

\textbf{Key Discovery:} Using ultra-low LR (2e-6, 25x lower than Phase 1) allowed the model to make microscopic adjustments while staying in the optimal region, achieving 63.93\% F1 with stable improvements. This demonstrates that the Phase 1 checkpoint had learned excellent representations but needed extremely gentle fine-tuning to unlock its full potential.


\paragraph{Qualitative Error Analysis:} 


\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{p{2.5cm}p{5.5cm}p{6cm}p{4.5cm}}
    \toprule
    \textbf{Error Type}& \textbf{Example Of Error} & \textbf{Error Description} & \textbf{Statistics} \\
    \midrule
    Missing Temporal Constraints  & \textbf{Query:} ``flights available tomorrow from Denver to Philadelphia'' \newline \textbf{Generated:} Missing date/day joins (no \texttt{days\_1}, \texttt{date\_day\_1} tables) & Model fails to capture temporal references like ``tomorrow'', ``Thursday'', ``morning''. The generated SQL omits necessary joins to \texttt{days} and \texttt{date\_day} tables to filter by specific dates/times. & High prevalence: $\sim$30-40\% of queries with temporal references (estimated 150/466 dev examples) \\
    
    \midrule
    Missing Time-of-Day Filters  & \textbf{Query:} ``afternoon flights from Washington to Boston'' \newline \textbf{Generated:} Missing \texttt{departure\_time BETWEEN 1200 AND 1800} clause & Model fails to translate time-of-day references (``morning'', ``afternoon'', ``before 9am'') into appropriate \texttt{departure\_time} or \texttt{arrival\_time} range constraints. & Medium prevalence: $\sim$20\% of queries with time-of-day (estimated 90/466) \\
    
    \midrule
    Incorrect Connecting Flight Logic  & \textbf{Query:} ``fly from Philadelphia to San Francisco through Dallas'' \newline \textbf{Generated:} Uses duplicate city constraints instead of \texttt{flight\_stop} table join & Model struggles with multi-leg journeys requiring \texttt{flight\_stop} table. Instead of joining on \texttt{flight\_stop.stop\_airport}, it incorrectly adds redundant destination city conditions. & Medium-high: $\sim$25\% of connecting flight queries (estimated 40/160) \\
    
    \midrule
    Over-simplified Schema  & Multiple examples generate basic city-to-city queries without constraints & Model often produces minimal viable queries that match schema structure but miss crucial filtering conditions (fares, times, specific airlines, aircraft types). This suggests difficulty handling complex multi-table joins and predicate composition. & Very high: $\sim$50-60\% of all queries (280/466) lack at least one constraint present in ground truth \\
    \bottomrule
  \end{tabular}
  \caption{Qualitative error analysis on dev set. Errors predominantly stem from incomplete semantic parsing of temporal/spatial constraints and difficulty with complex multi-table join patterns. The model successfully learns basic schema structure (city/airport joins) but struggles with compositional reasoning required for full query specification.}\label{tab:qualitative}
\end{table}
\end{landscape}

\section*{Q7.}

Provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted. 

\url{https://drive.google.com/file/d/1q8lVyqCE7kEfqCZLsR19WJ-sOQD62IXa/view?usp=sharing}

\textbf{Model Details:} T5-small fine-tuned with ultra-low learning rate (2e-6) strategy, achieving 63.93\% Record F1 and 56.44\% Record EM on development set. The model was trained in two phases: Phase 1 (cold start, LR=5e-5) reached 36.66\% F1, then Phase 2 (ultra-low LR=2e-6) unlocked the checkpoint's full potential by making microscopic weight adjustments while staying in the optimal region.\footnote{I used Cursor and ChatGPT to assist in documentation and to help verify the accuracy of my written code.}

\end{document}
